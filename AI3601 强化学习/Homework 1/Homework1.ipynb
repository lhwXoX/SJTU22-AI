{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI 3601 Homework 1 \n",
    "## Due: 23:59:59 (GMT +08:00), March 27, 2025 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. Convergence of Policy Iteration\n",
    "\n",
    "Given an MDP with finite state space $S$, finite actions space $A$, and rewards $R(s,a,s')$. We first define the $V$-value function and $Q$-value function with reward function corresponding to state $s$, action $a$ and next state $s'$. Specifically, the $V$-value function of policy $\\pi$ at state $s$ is defined as\n",
    "$$\n",
    "V^{\\pi}(s)=\\mathbb{E}\\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t,a_t, s'_t) \\mid \\pi,s_0=s \\right],\n",
    "$$\n",
    "\n",
    "and the $Q$-value of policy $\\pi$ at state $s$ and action $a$ is defined as\n",
    "$$\n",
    "Q^{\\pi}(s,a) = \\mathbb{E}\\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t,a_t, s'_t) \\mid \\pi,s_0=s,a_0=a \\right].\n",
    "$$\n",
    "\n",
    "Corresponding to the above $V$-value and $Q$-value function, recall that the policy iteration algorithm is equivalent to\n",
    "- **Policy Evaluation**: For fixed current policy $\\pi_i$, compute the $V$-values by iterating until values converge:\n",
    "    $$\n",
    "    V_{k+1}^{\\pi_i} \\leftarrow \\sum_{s'} P(s,\\pi_i(s),s')[R(s,\\pi_i(s),s')+\\gamma V_k^{\\pi_i}(s')],\n",
    "    $$\n",
    "    where $k$ denotes the iterating step when computing the values.\n",
    "- **Policy Improvement**: For fixed values, get a better policy using policy extraction:\n",
    "    $$\n",
    "    \\pi_{i+1}(s) \\in \\arg\\max_a \\sum_{s'} P(s,a,s')[R(s,a,s')+\\gamma V^{\\pi_i}(s')],\n",
    "    $$\n",
    "    which is equivalent to $\\pi_{i+1}(s) \\in \\arg\\max_a Q^{\\pi_i}(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Prove that a policy improvement step will always produce a new policy at least as good as the original one ($V^{\\pi_{i+1}}(s) \\geq V^{\\pi_{i}}(s)$ for any state $s$), and prove that policy iteration converges to an optimal policy. You can directly provide your answer below using Markdown, or write it by hand and submit an additional PDF file in Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "----\n",
    "\n",
    "Let $\\pi_i$ be the current policy and $\\pi_{i+1}$ be the policy after policy improvement. Since $\\pi_i$ may not be the optimal policy, we have\n",
    "$$\n",
    "V^{\\pi_i}(s)=Q^{\\pi_i}(s, \\pi_i(s))\\le \\max_a Q^{\\pi_i}(s,a).\n",
    "$$\n",
    "Expand $\\max_a Q^{\\pi_i}(s,a)$, we have\n",
    "$$\n",
    "\\max_a Q^{\\pi_i}(s,a) = \\max_a R(s,a) + \\gamma \\sum_{s'}T(s,a,s')V^{\\pi_i}(s').\n",
    "$$\n",
    "By the definition of $\\pi_{i+1}$, the above expression is equal to\n",
    "$$\n",
    "R(s,\\pi_{i+1}(s)) + \\gamma \\sum_{s'}T(s,\\pi_{i+1}(s),s')V^{\\pi_i}(s').\n",
    "$$\n",
    "As mentioned above, $\\pi_i$ may not be the optimal policy, we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "R(s,\\pi_{i+1}(s)) + \\gamma \\sum_{s'}T(s,\\pi_{i+1}(s),s')V^{\\pi_i}(s') &\\le R(s,\\pi_{i+1}(s)) + \\gamma \\sum_{s'}T(s,\\pi_{i+1}(s),s')(\\max_{a'}Q^{\\pi_i}(s',a'))\\\\\n",
    "&= R(s,\\pi_{i+1}(s)) + \\gamma \\sum_{s'}T(s,\\pi_{i+1}(s),s')(R(s', \\pi_{i+1}(s'))+\\gamma \\sum_{s''}T(s',\\pi_{i+1}(s'),s'')V^{\\pi_i}(s'')).\n",
    "\\end{align*}\n",
    "$$\n",
    "Then, repeat expanding $V^{\\pi_i}(s'')$ to a value $\\le \\max_{a''}Q^{\\pi_i}(s'',a'')$. Finally, we have\n",
    "$$\n",
    "R(s,\\pi_{i+1}(s)) + \\gamma \\sum_{s'}T(s,\\pi_{i+1}(s),s')(R(s', \\pi_{i+1}(s'))+\\gamma \\sum_{s''}T(s',\\pi_{i+1}(s'),s'')V^{\\pi_i}(s'')) \\le V^{\\pi_{i+1}}(s),\n",
    "$$\n",
    "which proves that $V^{\\pi_i}(s)\\le V^{\\pi_{i+1}}(s)$.\n",
    "\n",
    "----\n",
    "\n",
    "Define the Bellman operator $B$ as follows\n",
    "$$\n",
    "BV(s)=\\max_a \\sum_{s'}T(s,a,s')[R(s,a,s')+\\gamma V(s')].\n",
    "$$\n",
    "Let $\\parallel V(s)-V'(s)\\parallel_{\\infty} = \\max_s \\left|V(s)-V'(s)\\right|$ be the infinity norm. We prove the convergence by proving that $B$ is a Contraction on $V$ for $\\gamma <1$. First, we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\parallel BV(s)-BV'(s)\\parallel_{\\infty}&=\\max_s \\left|\\max_a \\sum_{s'}T(s,a,s')[R(s,a,s')+\\gamma V(s')] - \\max_a \\sum_{s'}T(s,a,s')[R(s,a,s')+\\gamma V'(s')]\\right|\\\\\n",
    "&\\le \\max_s \\max_a \\left|\\sum_{s'}T(s,a,s')[R(s,a,s')+\\gamma V(s')]-\\sum_{s'}T(s,a,s')[R(s,a,s')+\\gamma V'(s')]\\right|\\\\\n",
    "&=\\gamma \\max_s \\max_a \\left|\\sum_{s'}T(s,a,s')[V(s')-V'(s')]\\right|\\\\\n",
    "&\\le \\gamma \\max_s \\max_a \\sum_{s'}T(s,a,s')\\max_{s'}\\left|V(s')-V'(s')\\right|\\\\\n",
    "&=\\gamma \\parallel V(s)-V'(s)\\parallel_{\\infty},\n",
    "\\end{align*}\n",
    "$$\n",
    "which implies that Bellman operator $B$ is a Contraction on $V$. Now we prove that there exists a unique $V^*$ that satisfies $V^*(s)=BV^*(s)$, which means it is the optimal value, and $V_k(s)$ converges to $V^*(s)$ for any $k>0$. Notice that\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\parallel V_{k+1}(s)-V_k(s) \\parallel &=\\parallel BV_k(s)-BV_{k-1}(s)\\parallel\\\\\n",
    "&\\le \\gamma \\parallel V_k(s)-V_{k-1}(s)\\parallel \\le \\gamma^2 \\parallel V_{k-1}(s)-V_{k-2}(s)\\parallel \\le ... \\le \\gamma^{k-1} \\parallel V_2(s)-V_1(s)\\parallel,\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Epsilon-greedy, UCB and Thompson Sampling algorithms\n",
    "\n",
    "In this part, we will implement three algorithms in a multi-armed bandit environment.\n",
    "\n",
    "You are required to finish  <u>*three*</u>   coding exercises and answer <u>*two*</u> questions in the following notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setting -- Multi-armed bandits\n",
    "\n",
    "Consider a finite-armed stochastic bandit problem with $K$ arms where each arm $a$ has a stationary reward distribution $R(\\cdot|a)$. The expected reward of arm $a$ is defined by $Q(a)=\\mathbb{E}_{r\\sim R(\\cdot|a)}[r]$ and the optimal arm is $~a^{∗} =argmax_{a}Q(a)$.\n",
    "\n",
    "Each time $t$, we select one arm $a_{t}$ and obtain the reward $r_{t}\\sim R(\\cdot|a_t)$.\n",
    "\n",
    "The expected regret can be defined as follows:\n",
    "\n",
    "$$ Regret(t) = t \\cdot Q(a^{∗}) - E\\bigg[\\sum_{i=0}^{t} r_{t}\\bigg]  $$\n",
    "\n",
    "We need to design an algorithm selecting arm to minimize the $Regret(T)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "\n",
    "# Multi-armed bandits environment，we have three kind of settings based on the gap of mean rewards between arms \n",
    "# Here are three optional setting：\n",
    "\n",
    "# ['small_gap','medium_gap','large_gap']\n",
    "\n",
    "# small_gap: the gap of mean rewards between the best arms and second best arm are small. And similarly medium_gap and large_gap.\n",
    "\n",
    "########################################\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "class MultiArmedBandit:\n",
    "    def __init__(self, num_arms, avgs):\n",
    "        self.num_arms = num_arms\n",
    "        self.mus = avgs\n",
    "        self.sigmas = [1.0] * self.num_arms\n",
    "\n",
    "    def get_reward(self, arm):\n",
    "        reward = np.random.normal(\n",
    "            loc=self.mus[arm],\n",
    "            scale=self.sigmas[arm]\n",
    "        )\n",
    "        return reward\n",
    "\n",
    "    @property\n",
    "    def best_arm(self):\n",
    "        return np.argmax(self.mus)\n",
    "\n",
    "def build_mab(mab_name):\n",
    "    if mab_name == \"small_gap\":\n",
    "        return MultiArmedBandit(10, [0.6, 0.9, 0.95, 0.89, 0.8, 0.3, 0.4, 0.1, 0.85, 0.5])\n",
    "    if mab_name == \"medium_gap\":\n",
    "        return MultiArmedBandit(10, [0.6, 0.7, 0.95, 0.75, 0.7, 0.3, 0.4, 0.1, 0.65, 0.5])\n",
    "    if mab_name == \"large_gap\":\n",
    "        return MultiArmedBandit(10, [0.55, 0.35, 0.95, 0.45, 0.2, 0.3, 0.4, 0.1, 0.15, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this part, we try to implement three classic algorithms：(A) epsilon-greedy, (B) UCB and (C) Thompson Sampling algorithms in the MAB environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractAlgo:\n",
    "    def __init__(self, mab):\n",
    "        self.mab = mab\n",
    "        self.num_arms = self.mab.num_arms\n",
    "\n",
    "    def run(num_steps):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A) Implement the epsilon-greedy algorithm.\n",
    "\n",
    "At each round $t$, the agent explicitly explores a random arm with probability $ϵ$:\n",
    "\n",
    "$$\n",
    "a_{t} =\n",
    "\\begin{cases}\n",
    "argmax_{a}\\ \\widehat{Q}(a), \\ \\ \\ with \\ probability \\ 1 − ϵ\\\\\n",
    "random\\ arm, \\ \\ \\ with \\ probability\\ ϵ\n",
    "\\end{cases}\n",
    "$$\n",
    "where $\\widehat{Q}(a)$ is the sample mean of arm $a$.\n",
    "\n",
    "**Coding exercise.** Implement the epsilon-greedy algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EpsilonGreedy(AbstractAlgo):\n",
    "    def __init__(self, mab, epsilon):\n",
    "        super().__init__(mab)\n",
    "        self.num_pulls = np.ones(self.num_arms)\n",
    "        self.emp_means = np.zeros(self.num_arms)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def run(self, num_steps):\n",
    "        # `rewards[t]` records the received reward at round t (zero-indexed).\n",
    "        # `regrets[t]` records the corresponding regret at round t (zero-indexed).\n",
    "        #  Attention we will caculate cumulative_regrets via np.cumsum(regrets)\n",
    "        rewards = np.zeros(num_steps)\n",
    "        regrets = np.zeros(num_steps)\n",
    "\n",
    "        for t in range(num_steps):\n",
    "            # Stabilize the algorithm by exploring each arm once at the begining.\n",
    "            if t < self.num_arms:\n",
    "                arm = t\n",
    "                self.emp_means[arm] = self.mab.get_reward(arm)\n",
    "                continue\n",
    "            \n",
    "            \"\"\"START YOUR CODE\"\"\"\n",
    "\n",
    "\n",
    "            \"\"\"END\"\"\"\n",
    "\n",
    "            \n",
    "        # Reset.\n",
    "        self.num_pulls = np.ones(self.num_arms)\n",
    "        self.emp_means = np.zeros(self.num_arms)\n",
    "\n",
    "        return rewards, regrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B) Implement the upper confidence bound (UCB) algorithm. \n",
    "\n",
    "At each round $t$, the UCB algorithm chooses an arm based on the following equation:\n",
    "$$\n",
    "a_{t} = argmax_{a} (\\widehat{Q}(a) + β\\sqrt{\\frac{2logt}{N_t(a)}})\n",
    "$$\n",
    "\n",
    "where $\\widehat{Q}(a)$ is the sample mean of arm $a$, $N_t(a)$ is the visitation count of arm $a$ till round $t$,&ensp; and $β$ is a scaling factor (a hyper-parameter).\n",
    "\n",
    "**Coding exercise.** Implement the upper confidence bound (UCB) algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB(AbstractAlgo):\n",
    "    def __init__(self, mab, beta):\n",
    "        super().__init__(mab)\n",
    "        self.emp_means = np.zeros(self.num_arms)\n",
    "        self.num_pulls = np.ones(self.num_arms)\n",
    "        self.beta = beta\n",
    "\n",
    "    def run(self, num_steps):\n",
    "        # `rewards[t]` records the received reward at round t (zero-indexed).\n",
    "        # `regrets[t]` records the corresponding regret at round t (zero-indexed).\n",
    "        #  Attention we will caculate cumulative_regrets via np.cumsum(regrets)\n",
    "        rewards = np.zeros(num_steps)\n",
    "        regrets = np.zeros(num_steps)\n",
    "\n",
    "        for t in range(num_steps):\n",
    "            # Stabilize the algorithm by exploring each arm once at the begining.\n",
    "            if t < self.num_arms:\n",
    "                arm = t\n",
    "                self.emp_means[arm] = self.mab.get_reward(arm)\n",
    "                continue\n",
    "            \n",
    "            \"\"\"START YOUR CODE\"\"\"\n",
    "\n",
    "            \n",
    "            \"\"\"END\"\"\"\n",
    "\n",
    "            \n",
    "        # Reset.\n",
    "        self.num_pulls = np.ones(self.num_arms)\n",
    "        self.emp_means = np.zeros(self.num_arms)\n",
    "\n",
    "        return rewards, regrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C) Implement the Thompson Sampling algorithm. \n",
    "\n",
    "At each round $t$, the Thompson Sampling algorithm chooses an arm based on the following equation:\n",
    "$$\n",
    "a_{t} = argmax_{a} \\widetilde{Q}_a(t),\n",
    "$$\n",
    "\n",
    "where $\\widetilde{Q}_a(t) \\sim Gaussian(\\widehat{Q}_a(t), \\frac{1}{T_a(t)+1})$, $T_a(t)$ is the visitation count of arm $a$ till round $t$, $\\widehat{Q}(a)$ is the sample mean of arm $a$.\n",
    "\n",
    "**Coding exercise.** Implement the Thompson Sampling algorithm. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling(AbstractAlgo):\n",
    "    def __init__(self, mab):\n",
    "        super().__init__(mab)\n",
    "        self.num_pulls = np.ones(self.num_arms)\n",
    "        self.emp_means = np.zeros(self.num_arms)\n",
    "        \n",
    "\n",
    "    def run(self, num_steps):\n",
    "        # `rewards[t]` records the received reward at round t (zero-indexed).\n",
    "        # `regrets[t]` records the corresponding regret at round t (zero-indexed).\n",
    "        #  Attention we will caculate cumulative_regrets via np.cumsum(regrets)\n",
    "        rewards = np.zeros(num_steps)\n",
    "        regrets = np.zeros(num_steps)\n",
    "\n",
    "        for t in range(num_steps):\n",
    "            # Stabilize the algorithm by exploring each arm once at the begining.\n",
    "            if t < self.num_arms:\n",
    "                arm = t\n",
    "                self.emp_means[arm] = self.mab.get_reward(arm)\n",
    "                continue\n",
    "\n",
    "            \"\"\"START YOUR CODE\"\"\"\n",
    " \n",
    " \n",
    "            \"\"\"END\"\"\"\n",
    "            \n",
    "                        \n",
    "        # Reset.\n",
    "        self.num_pulls = np.ones(self.num_arms)\n",
    "        self.emp_means = np.zeros(self.num_arms)\n",
    "\n",
    "        return rewards, regrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running three algorithms in the multi-armed bandits environment\n",
    "\n",
    "Here, we try to run three algorithms in our environment. You should run the following code without editing and answer **Question 2, 3** based on the output. **Your output will be one of the criteria we use to score**.\n",
    "\n",
    "\n",
    "You can also change the hyperparameters privately to get a better understanding of these three algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1\n",
    "mad_name=['small_gap',\"medium_gap\",\"large_gap\"] # type of bandit setting\n",
    "num_iters=10000      # num of iterations\n",
    "num_simulations=30   # num of simulations\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "def run_the_algorithm(mad_name,num_iters,num_simulations):\n",
    "\n",
    "    rewards_env={}\n",
    "    regrets_env={}\n",
    "    \n",
    "    for mad in mad_name:\n",
    "        env = build_mab(mad)\n",
    "        agents = {\n",
    "        \"UCB with beta = 1.0\": UCB(env, 1.0),\n",
    "        \"UCB with beta = 2.0\": UCB(env, 2.0),\n",
    "        \"UCB with beta = 3.0\": UCB(env, 3.0),\n",
    "        \"Epsilon-greedy with epsilon = 0.1\": EpsilonGreedy(env, 0.1),\n",
    "        \"Epsilon-greedy with epsilon = 0.2\": EpsilonGreedy(env, 0.2),\n",
    "        \"Epsilon-greedy with epsilon = 0.3\": EpsilonGreedy(env, 0.3),\n",
    "        \"Thompson Sampling\": ThompsonSampling(env)\n",
    "        }\n",
    "        reward_records = {}\n",
    "        regret_records = {}\n",
    "\n",
    "        for name, agent in agents.items():\n",
    "            average_rewards_all = np.zeros((num_simulations, num_iters))\n",
    "            cumulative_regrets_all = np.zeros((num_simulations, num_iters))\n",
    "\n",
    "            for n in range(0,num_simulations):\n",
    "                rewards, regrets = agent.run(num_iters)\n",
    "                average_rewards = np.cumsum(rewards) / np.arange(1, num_iters + 1)\n",
    "                cumulative_regrets = np.cumsum(regrets)\n",
    "                average_rewards_all[n] = average_rewards\n",
    "                cumulative_regrets_all[n] = cumulative_regrets\n",
    "            reward_records[name] = np.mean(average_rewards_all, axis=0)\n",
    "            regret_records[name] = np.mean(cumulative_regrets_all, axis=0)\n",
    "        rewards_env[mad]=reward_records\n",
    "        regrets_env[mad]=regret_records\n",
    "    \n",
    "    return rewards_env,regrets_env\n",
    "\n",
    "rewards_env,regrets_env = run_the_algorithm(mad_name,num_iters,num_simulations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [i for i in range(0, num_iters)]\n",
    "fig=plt.figure(figsize=(15,7))\n",
    "\n",
    "for i in range(len(mad_name)):\n",
    "    fig1=plt.subplot(2,3,i+1)\n",
    "    plt.xlabel(\"Round\")\n",
    "    plt.ylabel(\"Expected average reward\")\n",
    "    for name, rewards in rewards_env[mad_name[i]].items():\n",
    "        plt.plot(x_axis, rewards,label=name)\n",
    "    plt.legend(prop={\"size\":7})\n",
    "    plt.title(mad_name[i]+'_'+'rewards')\n",
    "\n",
    "for i in range(len(mad_name)):\n",
    "    fig1=plt.subplot(2,3,i+4)\n",
    "    plt.xlabel(\"Round\")\n",
    "    plt.ylabel(\"Expected average regret\")\n",
    "    for name, regrets in regrets_env[mad_name[i]].items():\n",
    "        plt.plot(x_axis, regrets,label=name)\n",
    "    plt.legend(prop={\"size\":7})\n",
    "    plt.title(mad_name[i]+'_'+'regrets')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "for i in range(len(mad_name)):\n",
    "    for a in list(regrets_env[mad_name[i]].keys()):\n",
    "        print(mad_name[i],\"  \",a,\"  regrets at t=10000  \",regrets_env[mad_name[i]][a][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions 2.** The sub-optimality gap $\\Delta=Q(a^*)-\\max_{a\\neq a^*}Q(a)$ is an important feature of a bandit problem. Hence, our environment prove three sub-optimality gap: small gap, medium gap and large gap. Discuss your observations on the performance of three algorithms when facing different suboptimality gaps.\n",
    "\n",
    "**Questions 3.** What are the differences of perfermances among these three algorithms? Please briefly explain the respective advantages of each algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3. Value-iteration and Policy-iteration\n",
    "\n",
    "\n",
    "In this part, we will implement value iteration and policy iteration algorithms in a Grid World environment.\n",
    "\n",
    "You are required to finish  <u>*two*</u>   coding exercises and answer <u>*one*</u> question in the following notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setting -- Grid World\n",
    "\n",
    "Consider a known two-dimensional grid world environment. \n",
    "\n",
    "You will control an agent (the robot shown in the picture) in the environment to make it to the TERMINAL STATE. Each action has a probability of 20% to not behave as expected, as\n",
    "specified in $getTransitionStatesAndProbs()$. When the agent enters the TERMINAL STATE, it must take the special ‘exit’ action to get the final reward. \n",
    "\n",
    "Your goal is to get the maximal rewards during each episode.\n",
    "\n",
    "![](./pic.png)\n",
    "\n",
    "More detail about the environment is in $setting.py$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this part, we try to implement two algorithms：(A) value iteration algorithm and (B) policy iteration algorithm in the Grid World environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "from setting import*\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    An agent must define a getAction method, but may also define the\n",
    "    following methods which will be called if they exist:\n",
    "\n",
    "    def registerInitialState(self, state): # inspects the starting state\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, index=0):\n",
    "        self.index = index\n",
    "\n",
    "    def getAction(self, state):\n",
    "        util.utilraiseNotDefined()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A) Implement the value-iteration algorithm\n",
    "\n",
    "Recall the value iteration state update equation:\n",
    "\n",
    "$$V_{k+1}(s) \\leftarrow \\max_{a} \\sum_{s'}P(s'|s,a)[R(s,a,s')+\\gamma V_{k}(s')]$$\n",
    "\n",
    "**Coding exercise.** Implement a value iteration agent in $ValueIterationAgent$. Your value iteration agent is an offline planner, not a reinforcement learning agent, so the relevant hyperparameter is the maximum number of iterations of value iteration it should run in its initial planning phase.\n",
    "\n",
    "You are also required to enable early stopping for value iteration by checking whether the maximum change of values among the states is smaller than hyperparameter $ϵ$  in an iteration.\n",
    "\n",
    "Implement the following methods for $ValueIterationAgent$.\n",
    "\n",
    "• Method $runValueIteration$ computes the value function $self.values$ by running the value iteration algorithm.\n",
    "\n",
    "• Method $computeActionfromvalues(state)$ computes the best action according to the valuefunction given by $self.values$.\n",
    "\n",
    "• Method $computeQvalueFromvalues(state, action)$ returns the Q-value of the $(state, action)$ pair given by the value function self.values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ValueIterationAgent(Agent):\n",
    "    \"\"\"An agent that takes a Markov decision process on initialization\n",
    "    and runs value iteration for a given number of iterations.\n",
    "\n",
    "    Hint: Test your code with commands like `python main.py -a value -i 100 -k 10`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mdp, discount = 0.9, epsilon=0.001, iterations = 100):\n",
    "        \"\"\"\n",
    "          Your value iteration agent should take an mdp on\n",
    "          construction, run the indicated number of iterations\n",
    "          and then act according to the resulting policy.\n",
    "\n",
    "          Some useful mdp methods you will use:\n",
    "              mdp.getStates()\n",
    "              mdp.getPossibleActions(state)\n",
    "              mdp.getTransitionStatesAndProbs(state, action)\n",
    "              mdp.getReward(state, action, nextState)\n",
    "              mdp.isTerminal(state)\n",
    "        \"\"\"\n",
    "        self.mdp = mdp\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon  # For examing the convergence of value iteration\n",
    "        self.iterations = iterations # The value iteration will run AT MOST these steps\n",
    "        self.values = util.Counter() # You need to keep the record of all state values here\n",
    "        self.runValueIteration()\n",
    "\n",
    "    def runValueIteration(self):\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "        \n",
    "        \"\"\" END CODE HERE \"\"\"\n",
    "        return\n",
    "\n",
    "    def getValue(self, state):\n",
    "        \"\"\"Return the value of the state (computed in __init__).\"\"\"\n",
    "        return self.values[state]\n",
    "\n",
    "    def computeQValueFromValues(self, state, action):\n",
    "        \"\"\"Compute the Q-value of action in state from the value function stored in self.values.\"\"\"\n",
    "\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        value=0\n",
    "\n",
    "        \"\"\" END CODE HERE \"\"\"\n",
    "        return value\n",
    "\n",
    "    def computeActionFromValues(self, state):\n",
    "        \"\"\"The policy is the best action in the given state\n",
    "        according to the values currently stored in self.values.\n",
    "\n",
    "        You may break ties any way you see fit.  Note that if\n",
    "        there are no legal actions, which is the case at the\n",
    "        terminal state, you should return None.\n",
    "        \"\"\"\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        bestaction=None\n",
    "        \"\"\" END CODE HERE \"\"\"\n",
    "        return bestaction\n",
    "\n",
    "    def getPolicy(self, state):\n",
    "        return self.computeActionFromValues(state)\n",
    "\n",
    "    def getAction(self, state):\n",
    "        return self.computeActionFromValues(state)\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        return self.computeQValueFromValues(state, action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B) Implement the policy iteration algorithm.\n",
    "\n",
    "Let us recall the policy iteration algorithm as the first step. Given an MDP with a finite state space, action space, and reward function $R(s,a, s^{\\prime})$. We first define the $V$-value function and $Q$-value function with reward function corresponding to state $s$, action $a$ and next state $s^{\\prime}$. Specifically, the $V$-value of policy $\\pi$ at state $s$ is defined as\n",
    " $$\n",
    "     V^\\pi(s)=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R\\left(s_t, a_t, s_t^{\\prime}\\right) \\mid \\pi, s_0=s\\right]\\,,\n",
    " $$\n",
    " and the $Q$-value of policy $\\pi$ at state $s$ and action $a$ is defined as\n",
    " $$\n",
    "     Q^\\pi(s, a)=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R\\left(s_t, a_t, s_t^{\\prime}\\right) \\mid \\pi, s_0=s, a_0=a\\right]\\,.\n",
    " $$\n",
    "Corresponding to the above $V$-value and $Q$-value function, recall that the policy iteration algorithm is equivalent to\n",
    "\n",
    "• **Policy Evaluation:** For fixed current policy $\\pi_i$,\n",
    "    compute the $V$ values by iterating until values converge:\n",
    "           $$\n",
    "               V_{k+1}^{\\pi_i}(s) \\leftarrow \\sum_{s^{\\prime}} P\\left(s, \\pi_i(s), s^{\\prime}\\right)\\left[R\\left(s, \\pi_i(s), s^{\\prime}\\right)+\\gamma V_k^{\\pi_i}\\left(s^{\\prime}\\right)\\right]\\,,\n",
    "           $$\n",
    "           where $k$ denotes the iterating step when computing the values. \n",
    "\n",
    "• **Policy Improvement:** For fixed values, get a better policy using policy extraction:\n",
    "           $$\n",
    "                \\pi_{i+1}(s)\\in \\arg \\max _a \\sum_{s^{\\prime}} P\\left(s, a, s^{\\prime}\\right)\\left[R\\left(s, a, s^{\\prime}\\right)+\\gamma V^{\\pi_i}\\left(s^{\\prime}\\right)\\right] \n",
    "           $$\n",
    "           which is equivalent to $\\pi_{i+1}(s)\\in \\arg \\max _a Q^{\\pi_i}(s,a)$.\n",
    "\n",
    "**Coding exercise.** Implement a policy iteration agent in $PolicyIterationAgent$. \n",
    "\n",
    "Again, your policy iteration agent is an offline planner and the relevant hyperparameter is the maximum number of iterations of policy iteration, which is determined by hyperparameter iteration. Policy evaluation iterates until\n",
    "values converge, which is also determined by hyperparameter $ϵ$.\n",
    "\n",
    "If the policy does not change in the policy improvement phase, policy iteration stops early.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class PolicyIterationAgent(Agent):\n",
    "    \"\"\"An agent that takes a Markov decision process on initialization\n",
    "    and runs policy iteration for a given number of iterations..\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mdp, discount = 0.9, epsilon=0.001, iterations = 100):\n",
    "        self.mdp = mdp\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon  # For examing the convergence of policy iteration\n",
    "        self.iterations = iterations # The policy iteration will run AT MOST these steps\n",
    "        self.values = util.Counter() # You need to keep the record of all state values here\n",
    "        self.policy = dict()\n",
    "        self.runPolicyIteration()\n",
    "\n",
    "    def runPolicyIteration(self):\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "        \"\"\"END\"\"\"\n",
    "        return\n",
    "\n",
    "    def getValue(self, state):\n",
    "        \"\"\"Return the value of the state (computed in __init__).\"\"\"\n",
    "        return self.values[state]\n",
    "\n",
    "    def computeQValueFromValues(self, state, action):\n",
    "        \"\"\"Compute the Q-value of action in state from the value function stored in self.values.\"\"\"\n",
    "\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        value = 0\n",
    "        \n",
    "        \"\"\"END\"\"\"\n",
    "        return value\n",
    "\n",
    "    def computeActionFromValues(self, state):\n",
    "        \"\"\"The policy is the best action in the given state\n",
    "          according to the values currently stored in self.values.\n",
    "\n",
    "          You may break ties any way you see fit.  Note that if\n",
    "          there are no legal actions, which is the case at the\n",
    "          terminal state, you should return None.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        bestaction=None\n",
    "\n",
    "        \"\"\"END\"\"\"\n",
    "        return bestaction\n",
    "\n",
    "    def getPolicy(self, state):\n",
    "        return self.policy[state]\n",
    "\n",
    "    def getAction(self, state):\n",
    "        return self.policy[state]\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        return self.computeQValueFromValues(state, action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running two algorithms in the GRID-WORLD environment and  Visualization\n",
    "\n",
    "Here, we try to run two algorithms in our environment. You should run the following code without editing and answer **Questions 4** based on the output. **Your output will be one of the criteria we use to score**.\n",
    "\n",
    "\n",
    "You also can change the hyperparameters privately to get a better understanding of these two algorithms. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####hyperparameters#########\n",
    "np.random.seed(0)\n",
    "grid=\"BookGrid\"            #type of environment,\"BridgeGrid\"#\"BookGrid\"\n",
    "epsilon=0.3      \n",
    "agents=[\"policy\",\"value\"]  #optional policy ,value\n",
    "iters_list=[1,2,3,4]       # num of iterations\n",
    "discount=0.9\n",
    "episodes=50\n",
    "\n",
    "###############################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def runEpisode(agent, environment, discount, decision, message, pause, episode):\n",
    "    returns = 0\n",
    "    totalDiscount = 1.0\n",
    "    environment.reset()\n",
    "    if 'startEpisode' in dir(agent): agent.startEpisode()\n",
    "    message(\"BEGINNING EPISODE: \"+str(episode)+\"\\n\")\n",
    "    while True:\n",
    "\n",
    "        # DISPLAY CURRENT STATE\n",
    "        state = environment.getCurrentState()\n",
    "        # display(state)\n",
    "        pause()\n",
    "\n",
    "        # END IF IN A TERMINAL STATE\n",
    "        actions = environment.getPossibleActions(state)\n",
    "        if len(actions) == 0:\n",
    "            message(\"EPISODE \"+str(episode)+\" COMPLETE: RETURN WAS \"+str(returns)+\"\\n\")\n",
    "            return returns\n",
    "\n",
    "        # GET ACTION (USUALLY FROM AGENT)\n",
    "        action = decision(state)\n",
    "        if action == None:\n",
    "            raise 'Error: Agent returned None action'\n",
    "\n",
    "        # EXECUTE ACTION\n",
    "        nextState, reward = environment.doAction(action)\n",
    "        message(\"Started in state: \"+str(state)+\n",
    "                \"\\nTook action: \"+str(action)+\n",
    "                \"\\nEnded in state: \"+str(nextState)+\n",
    "                \"\\nGot reward: \"+str(reward)+\"\\n\")\n",
    "        # UPDATE LEARNER\n",
    "        if 'observeTransition' in dir(agent):\n",
    "            agent.observeTransition(state, action, nextState, reward)\n",
    "\n",
    "        returns += reward * totalDiscount\n",
    "        totalDiscount *= discount\n",
    "\n",
    "    if 'stopEpisode' in dir(agent):\n",
    "        agent.stopEpisode()\n",
    "\n",
    "\n",
    "\n",
    "# opts = parseOptions()\n",
    "\n",
    "###########################\n",
    "# GET THE GRIDWORLD\n",
    "###########################\n",
    "\n",
    "mdpFunction = eval(\"get\"+grid)\n",
    "mdp = mdpFunction()\n",
    "env = GridworldEnvironment(mdp)\n",
    "\n",
    "###########################\n",
    "# GET THE DISPLAY ADAPTER\n",
    "###########################\n",
    "\n",
    "\n",
    "def get_values_actions(a,num):\n",
    "    width=0\n",
    "    heigh=0\n",
    "    for i in range(len(list(a.values.keys()))):\n",
    "        if(len(list(a.values.keys())[i])==2):\n",
    "            if(width<list(a.values.keys())[i][0]+1):\n",
    "                width=list(a.values.keys())[i][0]+1\n",
    "            if(heigh<list(a.values.keys())[i][1]+1):\n",
    "                heigh=list(a.values.keys())[i][1]+1\n",
    "    # width=list(a.values.keys())[-1][0]+1\n",
    "    # heigh=list(a.values.keys())[-1][1]+1\n",
    "    vi=np.zeros((heigh,width))\n",
    "    action_x=np.zeros((heigh,width))\n",
    "    action_y=np.zeros((heigh,width))\n",
    "    # print(heigh,width)\n",
    "    for i in range(width):\n",
    "        for j in range(heigh):\n",
    "            \n",
    "            vi[heigh-1-j][i]=a.values[(i,j)]\n",
    "            if((i,j) not in a.policy ):\n",
    "                continue\n",
    "            if(a.policy[(i,j)]==\"east\"):\n",
    "                action_x[heigh-1-j][i]=num\n",
    "                action_y[heigh-1-j][i]=0\n",
    "            elif(a.policy[(i,j)]==\"south\"):\n",
    "                action_x[heigh-1-j][i]=0\n",
    "                action_y[heigh-1-j][i]=num\n",
    "            elif(a.policy[(i,j)]==\"west\"):\n",
    "                action_x[heigh-1-j][i]=-num\n",
    "                action_y[heigh-1-j][i]=0   \n",
    "            elif(a.policy[(i,j)]==\"north\"):\n",
    "                action_x[heigh-1-j][i]=0\n",
    "                action_y[heigh-1-j][i]=-num\n",
    "    return vi,action_x,action_y\n",
    "\n",
    "def plot_(value_mat,action_x,action_y,agent_type,env_name):\n",
    "    if(env_name==\"BookGrid\"):\n",
    "        vmin=-1\n",
    "        vmax=1\n",
    "    elif(env_name==\"BridgeGrid\"):\n",
    "        vmin=-100\n",
    "        vmax=10\n",
    "    matrix = value_mat\n",
    "\n",
    "    plt.figure(figsize=(value_mat.shape[1]*2,value_mat.shape[0]*2))\n",
    "    plt.imshow(matrix, cmap='viridis', interpolation='nearest',vmin=vmin, vmax=vmax)\n",
    "\n",
    "    for i in range(value_mat.shape[0]):\n",
    "        for j in range(value_mat.shape[1]):\n",
    "            plt.text(j , i , '{:.2f}'.format(matrix[i, j]), ha='center', va='center', color='red')\n",
    "            plt.arrow(j+action_x[i][j] , i+action_y[i][j] , action_x[i][j], action_y[i][j], color='black', width=0.02, head_width=0.05)\n",
    "    plt.title(agent_type+\"_value_action\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "###########################\n",
    "# RUN EPISODES\n",
    "###########################\n",
    "\n",
    "messageCallback = lambda x: printString(x)\n",
    "messageCallback= lambda x : None \n",
    "# FIGURE OUT WHETHER TO WAIT FOR A KEY PRESS AFTER EACH TIME STEP\n",
    "pauseCallback = lambda : None\n",
    "# RUN EPISODES\n",
    "\n",
    "\n",
    "sum_of_rewards_value=[]\n",
    "sum_of_rewards_policy=[]\n",
    "\n",
    "for iters in iters_list:\n",
    "    for agent in agents:\n",
    "        a = None\n",
    "        if agent == 'value':\n",
    "            a = ValueIterationAgent(mdp, discount, epsilon, iters)\n",
    "\n",
    "        elif agent == 'policy':\n",
    "            a = PolicyIterationAgent(mdp, discount, epsilon,iters)\n",
    "        else:\n",
    "            raise Exception('Unknown agent type: '+agent)\n",
    "        \n",
    "        decisionCallback = a.getAction\n",
    "\n",
    "        if episodes > 0:\n",
    "            print()\n",
    "            print(\"RUNNING\", episodes, \"EPISODES\")\n",
    "            print(\"AGENT\",agent)\n",
    "        returns = 0\n",
    "        for episode in range(1, episodes+1):\n",
    "            returns+= runEpisode(a, env, discount, decisionCallback, messageCallback, pauseCallback, episode)\n",
    "        if episodes > 0:\n",
    "            # print()\n",
    "            print(\"ITERS : \"+str(iters)+\"  ,AVERAGE SUM OF REWARDS FROM START STATE: \"+str((returns+0.0) / episodes))\n",
    "            print()\n",
    "            print()\n",
    "        if(agent==\"value\"):\n",
    "            sum_of_rewards_value.append((returns+0.0) / episodes)\n",
    "            a.policy={}\n",
    "            for key in list(a.values.keys()):\n",
    "                a.policy[key]=a.getPolicy(key)\n",
    "        else:\n",
    "            sum_of_rewards_policy.append((returns+0.0) / episodes)\n",
    "        \n",
    "        # if(iters==iters_list[-1]):\n",
    "        value_mat,action_x,action_y=get_values_actions(a,0.2)\n",
    "        plot_(value_mat,action_x,action_y,agent,grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions 4.**\n",
    "\n",
    "Observe the pictures of all states against the number of iterations for both $ValueIterationAgent$ and $PolicyIterationAgent$. \n",
    "\n",
    "Which algorithm converges faster?   (Which algorithm converges to optimal policy faster?) \n",
    "\n",
    "Explain your conclusion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
